{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3e09d-51f7-40d9-a60e-2ca340690c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df102395-1f4d-4919-877f-4dfd72a4d2b1",
   "metadata": {},
   "source": [
    "# Goodreads Books Reviews example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d4ae6-2a25-46d2-94a2-38afde7cbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_REVIEW_TRAIN_PATH = 'goodreads_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58879b-f302-453e-a3b3-6bcef375f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(BOOK_REVIEW_TRAIN_PATH, usecols=['review_id', 'review_text'], nrows=100000)\n",
    "train.set_index('review_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9d61b-37b2-4402-8cba-fb3136f74a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = train.sample(10).copy()\n",
    "subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c546798-d457-4b02-b5da-187e0919c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample.review_text.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af188d-6177-4ca7-9864-003a6a46845a",
   "metadata": {},
   "source": [
    "## Step 1: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809eb5a-100c-4218-9804-5fe8d3b425ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "punct = re.compile('[' + re.escape(string.punctuation) + ']')\n",
    "digits = re.compile('[' + re.escape(string.digits) + ']')\n",
    "\n",
    "def clean (str_):  \n",
    "    \n",
    "    str_ = str_.lower()\n",
    "    str_ = re.sub('\\n',' ',str_)\n",
    "    str_ = re.sub(punct,r' ', str_)\n",
    "    str_ = re.sub(digits,r' ', str_)\n",
    "    str_ = re.sub(r'\\s+',r' ', str_)\n",
    "    str_ = str_.strip()\n",
    "    \n",
    "    return str_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1660a-d09c-4a43-bbea-32515f0cd53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07acc68e-c222-45c7-825e-3777e5850d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample['review_text'].apply(clean).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9db6bc-6fe0-480f-9c77-0e38d169cd14",
   "metadata": {},
   "source": [
    "## Step 2: Reducing each word into a common base (root)\n",
    "https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7a52f-7eac-443b-906c-6ace381c3364",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Works by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes. <br> \n",
    "A stem can be the same for the inflectional forms of different lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d2b2a-add7-4ecc-a1f4-74de4aa9fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stem(str_):\n",
    "    \n",
    "    str_ = clean(str_)\n",
    "    \n",
    "    words = str_.strip().split(' ')\n",
    "    words = ' '.join([SnowballStemmer('english').stem(word) for word in words])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c503397-b2cb-40ec-b642-043c850c3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample['review_text_stem'] = subsample['review_text'].apply(stem)\n",
    "subsample['review_text_stem'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda6891-42e6-4786-a535-4c8ac33578d6",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Takes into consideration the morphological analysis of the words. <br>\n",
    "Same lemma can correspond to forms with different stems.<br>\n",
    "A lemma is the base form of all its inflectional forms, whereas a stem isnâ€™t.<br>\n",
    "\n",
    "Lemmatization is a common technique to increase recall (to make sure no relevant document gets lost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867811b-3280-450e-8707-c7382e382c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize (str_):\n",
    "    \n",
    "    str_ = clean(str_)\n",
    "    \n",
    "    words = str_.strip().split(' ')\n",
    "    words = ' '.join([lemmatizer.lemmatize(word) for word in words ])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee8c00-645b-4b81-b901-71c87d261c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample['review_text_lemme'] = subsample['review_text'].apply(lemmatize)\n",
    "subsample['review_text_lemme'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58734cd-5179-4fb2-8f20-f5aeb413c8da",
   "metadata": {},
   "source": [
    "## Step 4: Stopwords elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c5262-80a2-4862-96b3-955299e841f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy download en_core_web_sm \n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from spacy import load \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "en = load('en_core_web_sm')\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "sklearn_stopwords = _stop_words.ENGLISH_STOP_WORDS\n",
    "spacy_stopwords = en.Defaults.stop_words\n",
    "\n",
    "print(f'There are {len(nltk_stopwords)} stopwords in nltk list.')\n",
    "print(f'There are {len(sklearn_stopwords)} stopwords in sklearn list.')\n",
    "print(f'There are {len(spacy_stopwords)} stopwords in spacy list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b092c-4f0c-4960-8e98-bc98804f0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(str_, reduce_funct, stopwords):\n",
    "    \n",
    "    str_ = reduce_funct(str_)\n",
    "    words = str_.strip().split(' ')\n",
    "    \n",
    "    words = ' '.join([word for word in words if word not in stopwords])\n",
    "    \n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754fe30-f69f-4100-aaf7-48a5ec4eb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample['review_text_stem_no_sw'] = subsample['review_text'].apply(remove_stop_words, stem, args=[stem, spacy_stopwords])\n",
    "subsample['review_text_stem_no_sw'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cfdea-37f9-4e52-a070-6913a40724c6",
   "metadata": {},
   "source": [
    "## Step 5: Vectorizing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488d437-ac96-4db6-b01a-277d1a40bd7f",
   "metadata": {},
   "source": [
    "### Bag-of-words + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721ee38-3a7a-4fc2-a879-eedae1d671ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features=10\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(min_df=.05, max_df=.5, \n",
    "                                  token_pattern=r'\\w{2,}',\n",
    "                                  ngram_range=(1, 1),\n",
    "                                  max_features=max_features\n",
    "                                  )\n",
    "\n",
    "word_vectorizer.fit(subsample['review_text_stem_no_sw'])\n",
    "\n",
    "pd.DataFrame(word_vectorizer.transform(subsample['review_text_stem_no_sw']).todense(), \n",
    "             columns = word_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c86607-01f6-4d81-9c48-246c7898ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All transformations in place\n",
    "\n",
    "word_vectorizer_raw = TfidfVectorizer(min_df=.05, max_df=.5,\n",
    "                                      token_pattern=r'\\w{2,}',\n",
    "                                      stop_words=spacy_stopwords,\n",
    "                                      analyzer='word',\n",
    "                                      ngram_range=(1, 1),\n",
    "                                      max_features=max_features)\n",
    "\n",
    "word_vectorizer_raw.fit(subsample['review_text'])\n",
    "\n",
    "pd.DataFrame(word_vectorizer_raw.transform(subsample['review_text']).todense(), \n",
    "             columns = word_vectorizer_raw.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3606b-4a31-43ef-b4bb-bd25a51145e0",
   "metadata": {},
   "source": [
    "### Word2Vec embedding\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02342de-ddf2-48d9-9f64-d1095f95b5c5",
   "metadata": {},
   "source": [
    "#### Initial learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa53bb-da8f-4e79-be7f-b00c78d4c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample['review_words'] = subsample['review_text_stem_no_sw'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4b5e6-b80b-433f-9aec-6772f9e32536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(subsample['review_words'],\n",
    "                                   vector_size=10,\n",
    "                                   window=5, # context\n",
    "                                   min_count=2,\n",
    "                                   sg=0, # 0=CBOW, 1=Skip-gram\n",
    "                                   epochs=5)\n",
    "\n",
    "w2v_model.save('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addb928-b054-4630-ba77-0ac4681c793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "print(len(w2v_words))\n",
    "\n",
    "w2v_model.wv.index_to_key[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608e94b-99a2-490e-bd19-dfbacd5a3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.get_vector('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d684b-fd84-43cb-8ef6-ab4e82e3a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate review vectors based on the word vectors for each word in the given review\n",
    "\n",
    "word_vector = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_words]) for ls in subsample['review_words']], \n",
    "                       dtype=object)\n",
    "\n",
    "word_vector[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26431c72-f073-452b-990a-cd0563364562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not each word has a vector\n",
    "\n",
    "for i, v in enumerate(word_vector):\n",
    "    print(len(subsample['review_words'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27caa0e9-e1ca-4167-a403-d662ca44c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word_model_vectors):\n",
    "        self.word_model_vector = word_model_vectors\n",
    "        self.vector_size = word_model_vectors.vector_size\n",
    "\n",
    "    def fit(self): \n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):  \n",
    "        doc_word_vector = np.vstack([self.word_average(sent) for sent in docs])\n",
    "        return doc_word_vector\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model_vector.index_to_key:\n",
    "                mean.append(self.word_model_vector.get_vector(word))\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160767d2-97ea-4d3c-99ab-bf855d5834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2v_model.wv)\n",
    "\n",
    "words_w2v_vectorized = mean_embedding_vectorizer.transform(subsample['review_words'])\n",
    "\n",
    "words_w2v_vectorized[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d2783-80df-44cf-b564-4c3dff233505",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(words_w2v_vectorized):\n",
    "    print(len(subsample['review_words'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00234bf0-beba-43fe-a53f-8dd87c28d4f0",
   "metadata": {},
   "source": [
    "#### Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a64fac6-f5d2-47fd-9e92-758a0f89b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample2 = train.sample(10).copy()\n",
    "subsample2['review_text_stem'] = subsample2['review_text'].apply(stem)\n",
    "subsample2['review_text_stem_no_sw'] = subsample2['review_text'].apply(remove_stop_words, stem, args=[stem, spacy_stopwords])\n",
    "subsample2['review_words'] = subsample2['review_text_stem_no_sw'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92efe7f-ea1b-479c-8071-64d6d8343b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_reloaded = gensim.models.Word2Vec.load('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f23ec-56a0-4fa0-a459-0509c6de3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_reloaded.build_vocab(subsample2['review_words'], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b43b19-ee4b-49af-be3d-794403cf764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model_reloaded.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efb957-c8cb-4b27-9b0a-8430bf6dba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_reloaded.train(subsample2['review_words'], \n",
    "                         total_examples=w2v_model_reloaded.corpus_count, \n",
    "                         epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca67d54-4a87-4115-83f7-42c30e9b65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_reloaded.save('w2v_model_updated')\n",
    "\n",
    "w2v_model_reloaded.wv.save('w2v_model_vectors_updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c72ee-811e-40a6-a029-d9d558bfebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_vectors_reloaded = gensim.models.KeyedVectors.load('w2v_model_vectors_updated', mmap='r')\n",
    "w2v_model_vectors_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354f359-a9d9-4f55-b893-63dd3b7b450d",
   "metadata": {},
   "source": [
    "### Word2Vec + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79514b8f-f66b-49b2-8ae0-1de198f65882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model_vectors):\n",
    "\n",
    "        self.word_model_vectors = word_model_vectors\n",
    "        self.word_idf_weight = None\n",
    "        self.vector_size = word_model_vectors.vector_size\n",
    "\n",
    "    def fit(self, docs): \n",
    "        text_docs = []\n",
    "        for doc in docs:\n",
    "            text_docs.append(' '.join(doc))\n",
    "\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(text_docs)  # must be list of text strings\n",
    "\n",
    "        max_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
    "        \n",
    "        self.word_idf_weight = defaultdict(lambda: max_idf,\n",
    "                           [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, docs):  \n",
    "        doc_word_vector = np.vstack([self.word_average(sent) for sent in docs])\n",
    "        return doc_word_vector\n",
    "\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model_vectors.index_to_key:\n",
    "                mean.append(self.word_model_vectors.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
    "\n",
    "        if not mean: \n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328f702-d5da-4071-9740-14f35db9d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfEmbeddingVectorizer(w2v_model.wv)\n",
    "\n",
    "tfidf_vec.fit(subsample['review_words'])  # fit tfidf model first\n",
    "\n",
    "words_w2v_tfidf_vectorized = tfidf_vec.transform(subsample['review_words'])\n",
    "\n",
    "words_w2v_tfidf_vectorized[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158268b9-6e61-49e2-810f-ac74784c9fa7",
   "metadata": {},
   "source": [
    "### Pretrained GloVe model from gensim\n",
    "https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ff3cf-c354-4da6-ada0-b6c616e77e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and corpora available in gensim\n",
    "\n",
    "import json\n",
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()\n",
    "\n",
    "print(json.dumps(info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a581b2-b1da-4320-a8c1-ba2cd20bb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models available in gensim with short description\n",
    "\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:100] + '...',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5253b2-eeaf-4676-9273-ba8d302c6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model_vectors = api.load(\"glove-wiki-gigaword-50\") \n",
    "glove_model_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cce7e2-b6d0-4bbb-9f83-4decbda05b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_model_vectors.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b79a40-563c-4105-abac-6680f83fcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model_vectors.get_vector('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962d034-d570-4208-9b43-2d1e06dde371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mev_glove = MeanEmbeddingVectorizer(glove_model_vectors)\n",
    "\n",
    "words_glove_vectorized = mev_glove.transform(subsample['review_words'])\n",
    "\n",
    "words_glove_vectorized[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ae8dd-ee96-46c6-b947-6824051127d0",
   "metadata": {},
   "source": [
    "### BERT embeddings\n",
    "The BERT (Bidirectional Encoder Representations from Transformers) family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after. <br>\n",
    "https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer\n",
    "https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#1-loading-pre-trained-bert\n",
    "\n",
    "https://www.kaggle.com/code/colearninglounge/vectorization-embeddings-elmo-bert-gpt#Using-the-Transfomer-Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b6020-ecb9-4abc-89c6-74f2cc4a4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_reviews = tokenizer(subsample['review_text'].tolist(), padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "tokens_tensor = encoded_reviews['input_ids']\n",
    "segments_tensors = encoded_reviews['token_type_ids']\n",
    "\n",
    "encoded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d4fbe-4682-426d-985f-70cefcb0d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a81ac5-da41-4214-8e6d-ca1d25310f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69562b-8b85-4784-9357-09be97cc9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13ce21-bfac-4672-88dd-d81bda4a5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Number of layers: {len(hidden_states)} (initial embeddings + 12 BERT layers)')\n",
    "layer_i = 0\n",
    "\n",
    "print ('Number of batches', len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print ('Number of tokens', len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print ('Number of hidden units', len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57ac61-d7d7-423b-b818-ddd183c9a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `hidden_states` is a Python list.\n",
    "print('Type of hidden_states: ', type(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d411d-288d-4701-a25d-48f8e417c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30eff6-1744-476b-95a7-22f096b317de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b141df5-8ab3-442a-8317-457c583349a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `hidden_states` has shape [13 x 10 x 512 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [512 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 512 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298b206-b93a-4fde-9d1f-7064c2251200",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [torch.mean(hidden_states[-2][i], dim=0) for i in range(len(hidden_states[0]))]\n",
    "embeddings[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62538952-11c2-4afb-94ed-a24eb8eeebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70fa73-b57a-440a-b114-fb468b050746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
