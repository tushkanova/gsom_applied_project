{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd20ece-598c-4f57-9fd5-09bb52759537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b801e65-ce17-4a19-9a0f-4afdad829650",
   "metadata": {},
   "source": [
    "# Goodreads Books Reviews example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84dabe-795f-4f4d-9f2d-5517ba58eef2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9717588e-1af4-4e21-962f-8078eb7dac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_REVIEW_TRAIN_PATH = 'goodreads_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8757908-f7bb-4a52-91a2-7b7c64ba51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(BOOK_REVIEW_TRAIN_PATH, usecols=['review_id', 'review_text', 'rating'], nrows=100000)\n",
    "data.set_index('review_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e35464e8-97a3-4194-92cc-1490438fc0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, dfdbb7b0eb5a7e4c26d59a937e2e5feb to 1dd15605d2fbfe18a51ff3adeacaf7b9\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   rating       100000 non-null  int64 \n",
      " 1   review_text  100000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2efb1060-2130-43cc-a6dd-f68d606f1d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    35387\n",
       "5    30294\n",
       "3    20288\n",
       "2     7714\n",
       "1     3240\n",
       "0     3077\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08c69573-6da9-4677-b494-924efc9a1d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "punct = re.compile('[' + re.escape(string.punctuation) + ']')\n",
    "digits = re.compile('[' + re.escape(string.digits) + ']')\n",
    "\n",
    "def clean (str_):  \n",
    "    \n",
    "    str_ = str_.lower()\n",
    "    str_ = re.sub('\\n',' ',str_)\n",
    "    str_ = re.sub(punct,r' ', str_)\n",
    "    str_ = re.sub(digits,r' ', str_)\n",
    "    str_ = re.sub(r'\\s+',r' ', str_)\n",
    "    str_ = str_.strip()\n",
    "    \n",
    "    return str_\n",
    "\n",
    "def stem(str_):\n",
    "    \n",
    "    str_ = clean(str_)\n",
    "    \n",
    "    words = str_.strip().split(' ')\n",
    "    words = ' '.join([SnowballStemmer('english').stem(word) for word in words])\n",
    "    \n",
    "    return words\n",
    "\n",
    "def remove_stop_words(str_, reduce_funct, stopwords):\n",
    "    \n",
    "    str_ = reduce_funct(str_)\n",
    "    words = str_.strip().split(' ')\n",
    "    \n",
    "    words = ' '.join([word for word in words if word not in stopwords])\n",
    "    \n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "33a4706c-b6f0-457a-9fee-654720495e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_stem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dfdbb7b0eb5a7e4c26d59a937e2e5feb</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>this is a special book it start slow for about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a5d2c3628987712d0e05c4f90798eb67</th>\n",
       "      <td>3</td>\n",
       "      <td>Recommended by Don Katz. Avail for free in Dec...</td>\n",
       "      <td>recommend by don katz avail for free in decemb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ede853b14dc4583f96cf5d120af636f</th>\n",
       "      <td>3</td>\n",
       "      <td>A fun, fast paced science fiction thriller. I ...</td>\n",
       "      <td>a fun fast pace scienc fiction thriller i read...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  rating  \\\n",
       "review_id                                  \n",
       "dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n",
       "a5d2c3628987712d0e05c4f90798eb67       3   \n",
       "2ede853b14dc4583f96cf5d120af636f       3   \n",
       "\n",
       "                                                                        review_text  \\\n",
       "review_id                                                                             \n",
       "dfdbb7b0eb5a7e4c26d59a937e2e5feb  This is a special book. It started slow for ab...   \n",
       "a5d2c3628987712d0e05c4f90798eb67  Recommended by Don Katz. Avail for free in Dec...   \n",
       "2ede853b14dc4583f96cf5d120af636f  A fun, fast paced science fiction thriller. I ...   \n",
       "\n",
       "                                                                   review_text_stem  \n",
       "review_id                                                                            \n",
       "dfdbb7b0eb5a7e4c26d59a937e2e5feb  this is a special book it start slow for about...  \n",
       "a5d2c3628987712d0e05c4f90798eb67  recommend by don katz avail for free in decemb...  \n",
       "2ede853b14dc4583f96cf5d120af636f  a fun fast pace scienc fiction thriller i read...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review_text_stem'] = data['review_text'].apply(stem)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e44bf2-c1ed-4f83-9a69-25e19471e3eb",
   "metadata": {},
   "source": [
    "## Train-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e704aac9-3548-4817-bc50-cd49fa12b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['rating']), data.rating,\n",
    "                                                    random_state=42, stratify=data.rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e57e9-21e7-4d0b-b034-a7f2c97f9b1e",
   "metadata": {},
   "source": [
    "## TF-IDF pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4434016c-2edc-4a36-9941-5c8601da4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "results = []\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "def get_tfidf_results(param_dict, stopwords, clf, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    for params in tqdm(ParameterGrid(param_dict)):\n",
    "#       print(params)\n",
    "        pipe = Pipeline(steps = [('tfidf', TfidfVectorizer(min_df=params['min_df'], max_df=params['max_df'],\n",
    "                                                           token_pattern=r'[A-Za-z]{3,}',\n",
    "                                                           max_features=params['max_features'],\n",
    "                                                           stop_words=stopwords)),\n",
    "                                 #('to_dense', DenseTransformer()), \n",
    "                                 ('clf', clf)\n",
    "                                ]\n",
    "                           )\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        train_preds = pipe.predict(X_train)\n",
    "\n",
    "        results.append(dict(estimator=pipe,\n",
    "                            parameters=params,\n",
    "                            train_f1 = f1_score(y_true=y_train, y_pred=pipe.predict(X_train), average='micro'),\n",
    "                            test_f1 = f1_score(y_true=y_test, y_pred=pipe.predict(X_test), average='micro')\n",
    "        ))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ab6fe-0835-4e24-83b3-2e5764df7e6e",
   "metadata": {},
   "source": [
    "## Baseline - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c9313c80-c070-40c8-b5ab-14c604f9b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:19<00:00, 19.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'estimator': Pipeline(steps=[('tfidf',\n",
       "                   TfidfVectorizer(max_df=0.5, max_features=1000, min_df=0.01,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among', 'amongst',\n",
       "                                               'amount', 'an', 'and', 'another',\n",
       "                                               'any', ...},\n",
       "                                   token_pattern='[A-Za-z]{3,}')),\n",
       "                  ('clf', LogisticRegression(max_iter=500))]),\n",
       "  'parameters': {'max_df': 0.5, 'max_features': 1000, 'min_df': 0.01},\n",
       "  'train_f1': 0.5255333333333333,\n",
       "  'test_f1': 0.49612}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from spacy import load \n",
    "en = load('en_core_web_sm')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "spacy_stopwords = en.Defaults.stop_words\n",
    "\n",
    "params = dict(min_df=[.01,], max_df=[.5, ], max_features = [1000,])\n",
    "\n",
    "results = []\n",
    "\n",
    "get_tfidf_results(params, spacy_stopwords, LogisticRegression(max_iter=500), \n",
    "                  X_train.review_text_stem, y_train,\n",
    "                  X_test.review_text_stem, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c23cc-5bc6-4e25-bd40-aba342c6064b",
   "metadata": {},
   "source": [
    "## Efficient enough model - CatBoost\n",
    "https://catboost.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7bc8325e-080a-4d19-9e50-717623dafdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 1.5289701\ttotal: 729ms\tremaining: 6.56s\n",
      "1:\tlearn: 1.4419874\ttotal: 1.36s\tremaining: 5.46s\n",
      "2:\tlearn: 1.3965448\ttotal: 2.06s\tremaining: 4.8s\n",
      "3:\tlearn: 1.3708660\ttotal: 2.57s\tremaining: 3.86s\n",
      "4:\tlearn: 1.3554182\ttotal: 3.05s\tremaining: 3.05s\n",
      "5:\tlearn: 1.3401561\ttotal: 3.64s\tremaining: 2.43s\n",
      "6:\tlearn: 1.3257662\ttotal: 4.17s\tremaining: 1.79s\n",
      "7:\tlearn: 1.3152249\ttotal: 4.7s\tremaining: 1.17s\n",
      "8:\tlearn: 1.3088649\ttotal: 5.22s\tremaining: 580ms\n",
      "9:\tlearn: 1.3011017\ttotal: 5.7s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:17<00:00, 17.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'estimator': Pipeline(steps=[('tfidf',\n",
       "                   TfidfVectorizer(max_df=0.5, max_features=1000, min_df=0.01,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among', 'amongst',\n",
       "                                               'amount', 'an', 'and', 'another',\n",
       "                                               'any', ...},\n",
       "                                   token_pattern='[A-Za-z]{3,}')),\n",
       "                  ('clf', LogisticRegression(max_iter=500))]),\n",
       "  'parameters': {'max_df': 0.5, 'max_features': 1000, 'min_df': 0.01},\n",
       "  'train_f1': 0.5255333333333333,\n",
       "  'test_f1': 0.49612},\n",
       " {'estimator': Pipeline(steps=[('tfidf',\n",
       "                   TfidfVectorizer(max_df=0.5, max_features=1000, min_df=0.01,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among', 'amongst',\n",
       "                                               'amount', 'an', 'and', 'another',\n",
       "                                               'any', ...},\n",
       "                                   token_pattern='[A-Za-z]{3,}')),\n",
       "                  ('clf',\n",
       "                   <catboost.core.CatBoostClassifier object at 0x177ed3df0>)]),\n",
       "  'parameters': {'max_df': 0.5, 'max_features': 1000, 'min_df': 0.01},\n",
       "  'train_f1': 0.4508133333333333,\n",
       "  'test_f1': 0.44476}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "params = dict(min_df=[.01,], max_df=[.5, ], max_features = [1000,])\n",
    "\n",
    "get_tfidf_results(params, spacy_stopwords, CatBoostClassifier(iterations=10,\n",
    "                                                              random_state=42), \n",
    "                  X_train.review_text_stem, y_train,\n",
    "                  X_test.review_text_stem, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a7395-76e2-49e7-b91c-a0bb63c61860",
   "metadata": {},
   "source": [
    "## Efficient enough model - XGBoost\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "006558c8-ec1e-42ba-830c-96ee07724257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'estimator': Pipeline(steps=[('tfidf',\n",
       "                   TfidfVectorizer(max_df=0.5, max_features=1000, min_df=0.01,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among', 'amongst',\n",
       "                                               'amount', 'an', 'and', 'another',\n",
       "                                               'any', ...},\n",
       "                                   token_pattern='[A-Za-z]{3,}')),\n",
       "                  ('clf', LogisticRegression(max_iter=500))]),\n",
       "  'parameters': {'max_df': 0.5, 'max_features': 1000, 'min_df': 0.01},\n",
       "  'train_f1': 0.5255333333333333,\n",
       "  'test_f1': 0.49612},\n",
       " {'estimator': Pipeline(steps=[('tfidf',\n",
       "                   TfidfVectorizer(max_df=0.5, max_features=1000, min_df=0.01,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among', 'amongst',\n",
       "                                               'amount', 'an', 'and', 'another',\n",
       "                                               'any', ...},\n",
       "                                   token_pattern='[A-Za-z]{3,}')),\n",
       "                  ('clf',\n",
       "                   <catboost.core.CatBoostClassifier object at 0x177ed3df0>)]),\n",
       "  'parameters': {'max_df': 0.5, 'max_features': 1000, 'min_df': 0.01},\n",
       "  'train_f1': 0.4508133333333333,\n",
       "  'test_f1': 0.44476},\n",
       " {'estimator': Pipeline(steps=[('tfidf',\n",
       "                   TfidfVectorizer(max_df=0.5, max_features=1000, min_df=0.01,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among', 'amongst',\n",
       "                                               'amount', 'an', 'and', 'another',\n",
       "                                               'any', ...},\n",
       "                                   token_pattern='[A-Za-z]...\n",
       "                                 feature_types=None, gamma=0, gpu_id=-1,\n",
       "                                 grow_policy='depthwise', importance_type=None,\n",
       "                                 interaction_constraints='', learning_rate=1,\n",
       "                                 max_bin=256, max_cat_threshold=64,\n",
       "                                 max_cat_to_onehot=4, max_delta_step=0,\n",
       "                                 max_depth=2, max_leaves=0, min_child_weight=1,\n",
       "                                 missing=nan, monotone_constraints='()',\n",
       "                                 n_estimators=10, n_jobs=0, num_parallel_tree=1,\n",
       "                                 objective='multi:softprob', predictor='auto', ...))]),\n",
       "  'parameters': {'max_df': 0.5, 'max_features': 1000, 'min_df': 0.01},\n",
       "  'train_f1': 0.4570133333333333,\n",
       "  'test_f1': 0.4474}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "params = dict(min_df=[.01,], max_df=[.5, ], max_features = [1000,])\n",
    "\n",
    "get_tfidf_results(params, spacy_stopwords, XGBClassifier(n_estimators=10, max_depth=2, \n",
    "                                                         learning_rate=1, \n",
    "                                                         objective='binary:logistic'), \n",
    "                  X_train.review_text_stem, y_train,\n",
    "                  X_test.review_text_stem, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895c22c-37ba-4acd-bd28-4550b07c751a",
   "metadata": {},
   "source": [
    "## TF-IDF grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16f4fe22-75d8-4e34-bb52-feadfa8aed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "tf_idf_pipe = Pipeline(steps = [('tfidf', TfidfVectorizer(token_pattern=r'[A-Za-z]{3,}',\n",
    "                                                          stop_words=spacy_stopwords)),\n",
    "                                ('clf', CatBoostClassifier(random_state=42, iterations=10, verbose=False))\n",
    "                                ]\n",
    "                        )\n",
    "\n",
    "params = dict(tfidf__min_df=[.05,], \n",
    "                  tfidf__max_df=[.5,],\n",
    "                  tfidf__max_features = [1000,],\n",
    "                  clf__learning_rate=[.5, 1]\n",
    "                   )\n",
    "                \n",
    "                          \n",
    "grid = GridSearchCV(estimator=tf_idf_pipe,\n",
    "                    param_grid=params,\n",
    "                    scoring=make_scorer(f1_score, average='micro'),\n",
    "                    cv=3,\n",
    "                    refit=True,\n",
    "                    verbose=5\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4287e99b-85e0-4563-ba02-ff88ed732c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV 1/3] END clf__learning_rate=0.5, tfidf__max_df=0.5, tfidf__max_features=1000, tfidf__min_df=0.05;, score=0.436 total time=   4.5s\n",
      "[CV 2/3] END clf__learning_rate=0.5, tfidf__max_df=0.5, tfidf__max_features=1000, tfidf__min_df=0.05;, score=0.438 total time=   4.5s\n",
      "[CV 3/3] END clf__learning_rate=0.5, tfidf__max_df=0.5, tfidf__max_features=1000, tfidf__min_df=0.05;, score=0.437 total time=   4.4s\n",
      "[CV 1/3] END clf__learning_rate=1, tfidf__max_df=0.5, tfidf__max_features=1000, tfidf__min_df=0.05;, score=0.441 total time=   4.4s\n",
      "[CV 2/3] END clf__learning_rate=1, tfidf__max_df=0.5, tfidf__max_features=1000, tfidf__min_df=0.05;, score=0.440 total time=   4.5s\n",
      "[CV 3/3] END clf__learning_rate=1, tfidf__max_df=0.5, tfidf__max_features=1000, tfidf__min_df=0.05;, score=0.437 total time=   4.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words={&quot;&#x27;d&quot;, &quot;&#x27;ll&quot;,\n",
       "                                                                    &quot;&#x27;m&quot;, &quot;&#x27;re&quot;,\n",
       "                                                                    &quot;&#x27;s&quot;, &quot;&#x27;ve&quot;,\n",
       "                                                                    &#x27;a&#x27;,\n",
       "                                                                    &#x27;about&#x27;,\n",
       "                                                                    &#x27;above&#x27;,\n",
       "                                                                    &#x27;across&#x27;,\n",
       "                                                                    &#x27;after&#x27;,\n",
       "                                                                    &#x27;afterwards&#x27;,\n",
       "                                                                    &#x27;again&#x27;,\n",
       "                                                                    &#x27;against&#x27;,\n",
       "                                                                    &#x27;all&#x27;,\n",
       "                                                                    &#x27;almost&#x27;,\n",
       "                                                                    &#x27;alone&#x27;,\n",
       "                                                                    &#x27;along&#x27;,\n",
       "                                                                    &#x27;already&#x27;,\n",
       "                                                                    &#x27;also&#x27;,\n",
       "                                                                    &#x27;although&#x27;,\n",
       "                                                                    &#x27;always&#x27;,\n",
       "                                                                    &#x27;am&#x27;,\n",
       "                                                                    &#x27;among&#x27;,\n",
       "                                                                    &#x27;amongst&#x27;,\n",
       "                                                                    &#x27;amount&#x27;,\n",
       "                                                                    &#x27;an&#x27;, &#x27;and&#x27;,\n",
       "                                                                    &#x27;another&#x27;,\n",
       "                                                                    &#x27;any&#x27;, ...},\n",
       "                                                        token_pattern=&#x27;[A-Za-z]{3,}&#x27;)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        &lt;catboost.core.CatBoostClassifier object at 0x1775a2850&gt;)]),\n",
       "             param_grid={&#x27;clf__learning_rate&#x27;: [0.5, 1], &#x27;tfidf__max_df&#x27;: [0.5],\n",
       "                         &#x27;tfidf__max_features&#x27;: [1000],\n",
       "                         &#x27;tfidf__min_df&#x27;: [0.05]},\n",
       "             scoring=make_scorer(f1_score, average=micro), verbose=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words={&quot;&#x27;d&quot;, &quot;&#x27;ll&quot;,\n",
       "                                                                    &quot;&#x27;m&quot;, &quot;&#x27;re&quot;,\n",
       "                                                                    &quot;&#x27;s&quot;, &quot;&#x27;ve&quot;,\n",
       "                                                                    &#x27;a&#x27;,\n",
       "                                                                    &#x27;about&#x27;,\n",
       "                                                                    &#x27;above&#x27;,\n",
       "                                                                    &#x27;across&#x27;,\n",
       "                                                                    &#x27;after&#x27;,\n",
       "                                                                    &#x27;afterwards&#x27;,\n",
       "                                                                    &#x27;again&#x27;,\n",
       "                                                                    &#x27;against&#x27;,\n",
       "                                                                    &#x27;all&#x27;,\n",
       "                                                                    &#x27;almost&#x27;,\n",
       "                                                                    &#x27;alone&#x27;,\n",
       "                                                                    &#x27;along&#x27;,\n",
       "                                                                    &#x27;already&#x27;,\n",
       "                                                                    &#x27;also&#x27;,\n",
       "                                                                    &#x27;although&#x27;,\n",
       "                                                                    &#x27;always&#x27;,\n",
       "                                                                    &#x27;am&#x27;,\n",
       "                                                                    &#x27;among&#x27;,\n",
       "                                                                    &#x27;amongst&#x27;,\n",
       "                                                                    &#x27;amount&#x27;,\n",
       "                                                                    &#x27;an&#x27;, &#x27;and&#x27;,\n",
       "                                                                    &#x27;another&#x27;,\n",
       "                                                                    &#x27;any&#x27;, ...},\n",
       "                                                        token_pattern=&#x27;[A-Za-z]{3,}&#x27;)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        &lt;catboost.core.CatBoostClassifier object at 0x1775a2850&gt;)]),\n",
       "             param_grid={&#x27;clf__learning_rate&#x27;: [0.5, 1], &#x27;tfidf__max_df&#x27;: [0.5],\n",
       "                         &#x27;tfidf__max_features&#x27;: [1000],\n",
       "                         &#x27;tfidf__min_df&#x27;: [0.05]},\n",
       "             scoring=make_scorer(f1_score, average=micro), verbose=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(stop_words={&quot;&#x27;d&quot;, &quot;&#x27;ll&quot;, &quot;&#x27;m&quot;, &quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                             &quot;&#x27;ve&quot;, &#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;,\n",
       "                                             &#x27;across&#x27;, &#x27;after&#x27;, &#x27;afterwards&#x27;,\n",
       "                                             &#x27;again&#x27;, &#x27;against&#x27;, &#x27;all&#x27;,\n",
       "                                             &#x27;almost&#x27;, &#x27;alone&#x27;, &#x27;along&#x27;,\n",
       "                                             &#x27;already&#x27;, &#x27;also&#x27;, &#x27;although&#x27;,\n",
       "                                             &#x27;always&#x27;, &#x27;am&#x27;, &#x27;among&#x27;, &#x27;amongst&#x27;,\n",
       "                                             &#x27;amount&#x27;, &#x27;an&#x27;, &#x27;and&#x27;, &#x27;another&#x27;,\n",
       "                                             &#x27;any&#x27;, ...},\n",
       "                                 token_pattern=&#x27;[A-Za-z]{3,}&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 &lt;catboost.core.CatBoostClassifier object at 0x1775a2850&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words={&quot;&#x27;d&quot;, &quot;&#x27;ll&quot;, &quot;&#x27;m&quot;, &quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &quot;&#x27;ve&quot;, &#x27;a&#x27;, &#x27;about&#x27;,\n",
       "                            &#x27;above&#x27;, &#x27;across&#x27;, &#x27;after&#x27;, &#x27;afterwards&#x27;, &#x27;again&#x27;,\n",
       "                            &#x27;against&#x27;, &#x27;all&#x27;, &#x27;almost&#x27;, &#x27;alone&#x27;, &#x27;along&#x27;,\n",
       "                            &#x27;already&#x27;, &#x27;also&#x27;, &#x27;although&#x27;, &#x27;always&#x27;, &#x27;am&#x27;,\n",
       "                            &#x27;among&#x27;, &#x27;amongst&#x27;, &#x27;amount&#x27;, &#x27;an&#x27;, &#x27;and&#x27;,\n",
       "                            &#x27;another&#x27;, &#x27;any&#x27;, ...},\n",
       "                token_pattern=&#x27;[A-Za-z]{3,}&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x1775a2850&gt;</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words={\"'d\", \"'ll\",\n",
       "                                                                    \"'m\", \"'re\",\n",
       "                                                                    \"'s\", \"'ve\",\n",
       "                                                                    'a',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'across',\n",
       "                                                                    'after',\n",
       "                                                                    'afterwards',\n",
       "                                                                    'again',\n",
       "                                                                    'against',\n",
       "                                                                    'all',\n",
       "                                                                    'almost',\n",
       "                                                                    'alone',\n",
       "                                                                    'along',\n",
       "                                                                    'already',\n",
       "                                                                    'also',\n",
       "                                                                    'although',\n",
       "                                                                    'always',\n",
       "                                                                    'am',\n",
       "                                                                    'among',\n",
       "                                                                    'amongst',\n",
       "                                                                    'amount',\n",
       "                                                                    'an', 'and',\n",
       "                                                                    'another',\n",
       "                                                                    'any', ...},\n",
       "                                                        token_pattern='[A-Za-z]{3,}')),\n",
       "                                       ('clf',\n",
       "                                        <catboost.core.CatBoostClassifier object at 0x1775a2850>)]),\n",
       "             param_grid={'clf__learning_rate': [0.5, 1], 'tfidf__max_df': [0.5],\n",
       "                         'tfidf__max_features': [1000],\n",
       "                         'tfidf__min_df': [0.05]},\n",
       "             scoring=make_scorer(f1_score, average=micro), verbose=5)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train.review_text_stem, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9d62d4e0-b6bd-4211-852d-cb1b7762a4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.4266057 , 3.42375731]),\n",
       " 'std_fit_time': array([0.0436979 , 0.04431355]),\n",
       " 'mean_score_time': array([1.03686595, 1.03896268]),\n",
       " 'std_score_time': array([0.00725533, 0.00995766]),\n",
       " 'param_clf__learning_rate': masked_array(data=[0.5, 1],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__max_df': masked_array(data=[0.5, 0.5],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__max_features': masked_array(data=[1000, 1000],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__min_df': masked_array(data=[0.05, 0.05],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__learning_rate': 0.5,\n",
       "   'tfidf__max_df': 0.5,\n",
       "   'tfidf__max_features': 1000,\n",
       "   'tfidf__min_df': 0.05},\n",
       "  {'clf__learning_rate': 1,\n",
       "   'tfidf__max_df': 0.5,\n",
       "   'tfidf__max_features': 1000,\n",
       "   'tfidf__min_df': 0.05}],\n",
       " 'split0_test_score': array([0.43596, 0.44136]),\n",
       " 'split1_test_score': array([0.43816, 0.4398 ]),\n",
       " 'split2_test_score': array([0.43712, 0.43656]),\n",
       " 'mean_test_score': array([0.43708, 0.43924]),\n",
       " 'std_test_score': array([0.00089859, 0.0019992 ]),\n",
       " 'rank_test_score': array([2, 1], dtype=int32)}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "327cc49a-2e3d-44a3-9e1f-c412c1ba8266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6e953_row0_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #d65f5f 43.7%, transparent 43.7%);\n",
       "}\n",
       "#T_6e953_row1_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #d65f5f 43.9%, transparent 43.9%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6e953\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6e953_level0_col0\" class=\"col_heading level0 col0\" >params</th>\n",
       "      <th id=\"T_6e953_level0_col1\" class=\"col_heading level0 col1\" >mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6e953_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6e953_row0_col0\" class=\"data row0 col0\" >{'clf__learning_rate': 0.5, 'tfidf__max_df': 0.5, 'tfidf__max_features': 1000, 'tfidf__min_df': 0.05}</td>\n",
       "      <td id=\"T_6e953_row0_col1\" class=\"data row0 col1\" >0.437080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e953_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6e953_row1_col0\" class=\"data row1 col0\" >{'clf__learning_rate': 1, 'tfidf__max_df': 0.5, 'tfidf__max_features': 1000, 'tfidf__min_df': 0.05}</td>\n",
       "      <td id=\"T_6e953_row1_col1\" class=\"data row1 col1\" >0.439240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x28ed2fd00>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score']].style.bar(vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0b1d1a32-a764-434d-84db-7777df7d1ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score: 0.44012\n"
     ]
    }
   ],
   "source": [
    "test_pred_gs = grid.best_estimator_.predict(X_test.review_text_stem)\n",
    "print(f\"Test f1 score: {f1_score(y_true=y_test, y_pred=test_pred_gs, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbd8e6-c270-4b87-8b82-a9161b194111",
   "metadata": {},
   "source": [
    "## Pretrained GloVe + CatBoost grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9871ae0e-47f6-48c7-b418-7eb8437607fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word_model_vectors):\n",
    "        self.word_model_vector = word_model_vectors\n",
    "        self.vector_size = word_model_vectors.vector_size\n",
    "\n",
    "    def fit(self): \n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):  \n",
    "        doc_word_vector = np.vstack([self.word_average(sent) for sent in docs])\n",
    "        return doc_word_vector\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model_vector.index_to_key:\n",
    "                mean.append(self.word_model_vector.get_vector(word))\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
    "mev_glove = MeanEmbeddingVectorizer(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aecab673-d271-4d84-b1f7-2d86ffd7505b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tk/zw8myvyj39z_q3c256dtl3d00000gn/T/ipykernel_47199/381710653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m X_train_vectorized = mev_glove.transform(X_train.review_text.apply(remove_stop_words, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                    \u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                    args=[stem, spacy_stopwords]).str.split(' '))\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m X_test_vectorized = mev_glove.transform(X_test.review_text.apply(remove_stop_words, \n",
      "\u001b[0;32m/var/folders/tk/zw8myvyj39z_q3c256dtl3d00000gn/T/ipykernel_47199/271376878.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdoc_word_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_word_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tk/zw8myvyj39z_q3c256dtl3d00000gn/T/ipykernel_47199/271376878.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdoc_word_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_word_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tk/zw8myvyj39z_q3c256dtl3d00000gn/T/ipykernel_47199/271376878.py\u001b[0m in \u001b[0;36mword_average\u001b[0;34m(self, sent)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mword_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_model_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_model_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_vectorized = mev_glove.transform(X_train.review_text.apply(remove_stop_words, \n",
    "                                                                   stem, \n",
    "                                                                   args=[stem, spacy_stopwords]).str.split(' '))\n",
    "\n",
    "X_test_vectorized = mev_glove.transform(X_test.review_text.apply(remove_stop_words, \n",
    "                                                                 stem, \n",
    "                                                                 args=[stem, spacy_stopwords]).str.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d155d-b469-40f1-977f-6c93cb03da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(learning_rate=[.5, 1])\n",
    "                \n",
    "grid = GridSearchCV(estimator=CatBoostClassifier(random_state=42, iterations=10, verbose=False),\n",
    "                    param_grid=params,\n",
    "                    scoring=make_scorer(f1_score, average='micro'),\n",
    "                    cv=3,\n",
    "                    refit=True,\n",
    "                    verbose=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5df090-990e-446c-b996-d4d240a572ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train_vectorized, y_train)\n",
    "pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score']].style.bar(vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c03938-9fef-43e1-b462-e4079ef48d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = grid.best_estimator_.predict(X_test.review_text_stem)\n",
    "print(f\"Test f1 score: {f1_score(y_true=y_test, y_pred=test_predictions, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e486b-63ee-40fe-96ef-436021c4b652",
   "metadata": {},
   "source": [
    "## BERT fine tuning\n",
    "https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model <br>\n",
    "https://huggingface.co/docs/transformers/model_doc/distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b3f21e0f-7533-4d2f-b0a3-af3dfa6b81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train['review_text'].tolist(), padding='max_length', truncation=True)\n",
    "X_test_tokenized = tokenizer(X_test['review_text'].tolist(), padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6fafd953-3670-4d3d-8152-b59872cd91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "test_dataset = Dataset(X_test_tokenized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7b30c186-953e-4210-b337-9f9a8e397cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/olko/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_params = TrainingArguments(output_dir='test_trainer', evaluation_strategy='epoch')\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "#f1_metric.compute(predictions=np.argmax(eval_pred[0], axis=-1), references=eval_pred[1])\n",
    "\n",
    "def compute_f1(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    return f1_metric.compute(predictions=np.argmax(logits, axis=-1), references=labels, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d4457bcb-f819-4ddd-b98a-2536282fe12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=bert_model,\n",
    "                  args=training_params,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=test_dataset,\n",
    "                  compute_metrics=compute_f1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5ab14ba-bdd5-4b15-96ff-70445fd79a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 75000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28125\n",
      "  Number of trainable parameters = 109486854\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1112' max='28125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1112/28125 2:41:36 < 65:32:58, 0.11 it/s, Epoch 0.12/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Configuration saved in test_trainer/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-1000\n",
      "Configuration saved in test_trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tk/zw8myvyj39z_q3c256dtl3d00000gn/T/ipykernel_47199/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         )\n\u001b[0;32m-> 1501\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1502\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587edcd-cf14-49c6-8dbf-74f338bcfba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
